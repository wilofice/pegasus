version: '3.8'

services:
  # Qdrant vector database (replacing ChromaDB)
  qdrant:
    image: qdrant/qdrant:latest
    container_name: pegasus-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__LOG_LEVEL=INFO
    restart: unless-stopped

  # Neo4j graph database
  neo4j:
    image: neo4j:5-community
    container_name: pegasus-neo4j
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
    environment:
      - NEO4J_AUTH=neo4j/pegasus123
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_dbms_memory_heap_max__size=1G
    restart: unless-stopped

  # Redis for caching and session management
  redis:
    image: redis:7-alpine
    container_name: pegasus-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    restart: unless-stopped

  # PostgreSQL database
  postgres:
    image: postgres:15-alpine
    container_name: pegasus-postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_DB=pegasus
      - POSTGRES_USER=pegasus
      - POSTGRES_PASSWORD=pegasus123
    restart: unless-stopped

  # Ollama for local LLM inference
  ollama:
    image: ollama/ollama:latest
    container_name: pegasus-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  qdrant_storage:
  neo4j_data:
  neo4j_logs:
  redis_data:
  postgres_data:
  ollama_models:

networks:
  default:
    name: pegasus-network