Implementation Plan: Pegasus Learning Game Feature
This document outlines the tasks required to implement the interactive learning game within Pegasus, following a refined, conversational approach.

on the Pegasus app, I want to add a gaming feature for learning . 

It is simple: we will have one main screen dedicated to show Multiple choices based questions . The user will have to select multiple responses or one. It can also input a custom answer in text input fields; All the data for the questions will come from the backend. The backend will send all the questions in one shot. Exemple : one call from the frontend to the backend will return 50 or 100 questions. The frontend will manage the state of the answered questions to displayed a final summary to the user. The frontend will display one question by question . There is only one particularity : the backend will be responsible to validate each response of the user and sending back the final answer. Basically, the questions are generated by an LLM in the backend api and the user responses are validated by the LLM; 

---------

Refine the feature. If something I said can be done in a better way , change it also the most recommended option. Then Write the full details of implementations task by task in markdown file; Each task must be very clear : input, output, guidelines , expecations, testing.

The backend is fast api based and the frontend is flutter based app. Listen this app is personal. I will be the only user. so no need to complicate authentication or such kind of thing at this step

Part 1: API & Data Model Design
Objective: Define the "contract" between the Flutter frontend and the FastAPI backend. This is the foundational step before any code is written.

#

Tâche

Description Détaillée

1.1

Définition des Modèles de Données

Définir les structures de données (en Pydantic pour le backend, en Dart pour le frontend) pour assurer la cohérence. 

 Modèle Question: 
 { "id": "q1", "text": "...", "type": "SINGLE_CHOICE", "options": ["A", "B", "C"] } 

 Modèle UserAnswer: 
 { "question_id": "q1", "selected_options": ["B"], "custom_text": null } 

 Modèle ValidationResponse: 
 { "is_correct": true, "explanation": "Excellent! C'est la bonne réponse parce que..." }

1.2

Conception des Endpoints API

Définir les routes de l'API qui géreront le flux du jeu. 

 1. POST /game/start 
    - Input: { "topic": "Histoire de l'Italie", "length": 10 } (Sujet et nombre de questions souhaitées) 
    - Output: La première Question. 

 2. POST /game/answer/{session_id} 
    - Input: UserAnswer 
    - Output: `{ "validation": ValidationResponse, "next_question": Question

Partie 2: Backend Implementation (FastAPI)
Objectif: Construire la logique serveur qui génère les questions, gère l'état du jeu et valide les réponses en utilisant un LLM.

#

Tâche

Description Détaillée

Input/Output & Testing

2.1

Mise en Place de l'Environnement

- Créer game_router.py dans le dossier api/. 
 - Créer les modèles Pydantic pour les Question, UserAnswer, etc. dans un nouveau fichier core/game_models.py.

Output: Structure de fichiers prête. 
 Testing: Le serveur doit démarrer sans erreur.

2.2

Gestion de l'État de la Session

- Créer un gestionnaire de session simple (un dictionnaire en mémoire est suffisant pour une app personnelle). 
 game_sessions = { "session_123": {"topic": "...", "questions_asked": [], "score": 0} } 
 - Implémenter la logique de création et de récupération de session.

Input: Un session_id. 
 Output: Les données de la session correspondante. 
 Testing: Tests unitaires pour créer, mettre à jour et récupérer une session.

2.3

Prompt Engineering pour le LLM

- Prompt pour la Génération de Question : Concevoir un prompt qui demande au LLM de générer une question sur un sujet donné, en tenant compte des questions précédentes pour varier les sujets. Le prompt doit demander une sortie en format JSON strict. 
 - Prompt pour la Validation : Concevoir un prompt qui donne au LLM la question, la réponse de l'utilisateur, et lui demande de juger si c'est correct et de fournir une brève explication.

Input: Un sujet, l'historique de la conversation. 
 Output: Un JSON formaté pour la question ou la validation. 
 Testing: Tester les prompts dans un playground LLM pour affiner leur qualité.

2.4

Implémentation des Endpoints API

- POST /game/start : Appelle le LLM pour générer la première question, crée une session de jeu et renvoie la question. 
 - POST /game/answer/{session_id} : Récupère la session, appelle le LLM pour valider la réponse, met à jour le score, appelle le LLM pour générer la question suivante, et renvoie la validation et la nouvelle question. 
 - GET /game/summary/{session_id} : Récupère la session et formate le résumé final.

Input: Requêtes HTTP conformes à la conception de l'API. 
 Output: Réponses JSON conformes. 
 Testing: Utiliser des outils comme Postman ou curl pour tester chaque endpoint de manière exhaustive.

Partie 3: Frontend Implementation (Flutter)
Objectif: Créer une interface utilisateur fluide et réactive pour le jeu, gérer l'état de la session et communiquer avec le backend.

#

Tâche

Description Détaillée

Input/Output & Testing

3.1

Mise en Place de l'Environnement

- Créer les nouveaux fichiers : game_screen.dart, game_provider.dart, et les modèles de données Dart correspondants. 
 - Ajouter les dépendances Flutter nécessaires si de nouvelles sont requises.

Output: Structure de fichiers Flutter prête.

3.2

Gestion de l'État du Jeu

- Dans game_provider.dart (en utilisant Riverpod), gérer l'état de l'écran de jeu : 
 isLoading (attente de la réponse du backend), currentQuestion, userSelection (ce que l'utilisateur coche/écrit), lastValidationResult.

Input: Actions de l'utilisateur (sélection d'une réponse). 
 Output: Mises à jour de l'état qui déclenchent des reconstructions de l'UI. 
 Testing: Tests unitaires sur le provider pour s'assurer que l'état change correctement.

3.3

Développement de l'UI de Jeu

- Dans game_screen.dart, construire une UI qui s'adapte au type de currentQuestion. 
 - Utiliser des widgets RadioListTile pour SINGLE_CHOICE. 
 - Utiliser des CheckboxListTile pour MULTIPLE_CHOICE. 
 - Utiliser un TextField pour FREE_TEXT. 
 - Créer un widget pour afficher le résultat de la validation (ex: une bannière verte/rouge qui apparaît après avoir répondu).

Input: L'objet currentQuestion de l'état. 
 Output: Une interface de question interactive. 
 Testing: Tests de widgets pour chaque type de question.

3.4

Intégration API

- Dans pegasus_api_client.dart, ajouter les nouvelles fonctions pour appeler les endpoints /game/start et /game/answer. 
 - Connecter les actions de l'UI (ex: appui sur le bouton "Valider") à ces fonctions API. 
 - Gérer les états de chargement (isLoading = true avant l'appel API, false après) et les erreurs potentielles.

Input: Données de l'utilisateur. 
 Output: Appels réseau vers le backend. 
 Testing: Tester le flux complet : démarrer un jeu, répondre à une question, et voir l'UI se mettre à jour avec la nouvelle question reçue du backend.

3.5

Développement de l'Écran de Résumé

- Créer un nouvel écran, game_summary_screen.dart. 
 - Quand le jeu est terminé, naviguer vers cet écran. 
 - Cet écran appellera l'endpoint /game/summary/{session_id} et affichera le score et les détails de manière claire et gratifiante.

Input: Un session_id. 
 Output: Un écran de résumé visuellement plaisant. 
 Testing: Naviguer manuellement vers cet écran avec un ID de session de test pour vérifier l'affichage.

